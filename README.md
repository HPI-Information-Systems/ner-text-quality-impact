# The Effect of Data Quality on Named Entity Recognition

With an upsurge of data in the digital world, it is crucial to identify and extract relevant information. This task requires a machine learning model to work effectively with unstructured data. Named entity recognition systems are a popular choice for these information extraction processes. These models are designed specifically for this purpose and can filter predefined entities from large volumes of textual data. Due to their high popularity and increasing demand, it becomes essential to recognize the factors that can compromise the performance of these models.

The impact of data annotation errors on NER models has been examined by various studies, leaving the broader implication of overall data quality on these models to be explored. In this work, the robustness of three prominent NER models is evaluated on datasets with varying amounts and types of noises. The results show that as the noise in the dataset increases, model performance declines, with a minor impact for some noise types and a significant drop in performance for others.

This study provides insight into the importance of data quality by highlighting the impact of different noise types and amounts on the predictive capacity of the various NER models. The results suggest that a modelâ€™s performance and data quality are directly connected. Therefore, it is necessary to apply proper data quality checks before feeding any training or testing data to a model. These checks on the early stages of an application can save resources, avoid wrong interpretations, and build named entity recognition systems that consistently perform well.